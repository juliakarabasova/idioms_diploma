{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9517cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncorrectURLError(Exception):\n",
    "    \"\"\"\n",
    "    Custom error\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class UnknownConfigError(Exception):\n",
    "    \"\"\"\n",
    "    Most general error\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236f5df",
   "metadata": {},
   "source": [
    "# Literature parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageParser:\n",
    "    def __init__(self, url, articles_per_page):\n",
    "        self.url = url\n",
    "        self.articles_num = articles_per_page\n",
    "\n",
    "        self.urls = []\n",
    "        self.soup = None\n",
    "        self.id_stop = 0\n",
    "\n",
    "    def parse(self):\n",
    "        response = requests.get(self.url, headers=HEADERS)\n",
    "        if not response:\n",
    "            raise IncorrectURLError(self.url)\n",
    "        page_soup = BeautifulSoup(response.content, features='lxml')\n",
    "        self.soup = page_soup.find('body')\n",
    "\n",
    "    def find_articles(self):\n",
    "        self.parse()\n",
    "        articles_soup = self.soup.find_all('a', class_='poemlink')\n",
    "        for article in articles_soup[:self.articles_num]:\n",
    "            self.urls.append('https://proza.ru'+article.attrs['href'])\n",
    "        else:\n",
    "            self.id_stop = len(self.urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleParser:\n",
    "    def __init__(self, url, article_id):\n",
    "        self.url = url\n",
    "        self.article_id = article_id\n",
    "\n",
    "        self.title = ''\n",
    "        self.author = ''\n",
    "        self.text = []\n",
    "\n",
    "    def parse(self):\n",
    "        response = requests.get(self.url, headers=HEADERS)\n",
    "        if not response:\n",
    "            raise IncorrectURLError\n",
    "        page_soup = BeautifulSoup(response.content, features='lxml')\n",
    "        article_soup = page_soup.find('body')\n",
    "\n",
    "        self.title = article_soup.find('index').find('h1').contents[0]\n",
    "        self.author = article_soup.find('div', class_='titleauthor').find('em').find('a').contents[0]\n",
    "        text = article_soup.find('div', class_='text').contents\n",
    "        self.text = [str(piece).replace('\\n', '').replace('\\t', '') for piece in text\n",
    "                     if str(piece)[0] != '<' and str(piece) != '\\n']\n",
    "\n",
    "    def write_info(self):\n",
    "        article_meta_name = \"{}_meta.json\".format(self.article_id)\n",
    "        article_txt_name = \"{}_raw.txt\".format(self.article_id)\n",
    "\n",
    "        with open(os.path.join(ASSETS_PATH, article_txt_name), 'w', encoding='utf-8') as file:\n",
    "            file.write(''.join(self.text))\n",
    "\n",
    "        meta = {\n",
    "                'id': self.article_id,\n",
    "                'url': self.url,\n",
    "                'title': self.title,\n",
    "                'author': self.author,\n",
    "            }\n",
    "\n",
    "        with open(os.path.join(ASSETS_PATH, article_meta_name), \"w\", encoding='utf-8') as file:\n",
    "            json.dump(meta,\n",
    "                      file,\n",
    "                      sort_keys=False,\n",
    "                      indent=4,\n",
    "                      ensure_ascii=False,\n",
    "                      separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_environment(base_path):\n",
    "    \"\"\"\n",
    "    Creates ASSETS_PATH folder if not created and removes existing folder\n",
    "    \"\"\"\n",
    "    shutil.rmtree(base_path, ignore_errors=True)\n",
    "    try:\n",
    "        os.makedirs(base_path, mode=0o777)\n",
    "    except OSError as error:\n",
    "        raise UnknownConfigError from error\n",
    "\n",
    "\n",
    "def generate_links(base_link):\n",
    "    links = [base_link]\n",
    "    for i in range(2, 13):\n",
    "        if i < 10:\n",
    "            i = '0'+str(i)\n",
    "        links.append(base_link.replace('month=01', f'month={str(i)}'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))\n",
    "ASSETS_PATH = os.path.join(PROJECT_ROOT, 'articles')\n",
    "#prepare_environment(ASSETS_PATH)\n",
    "\n",
    "#URL_HEAD = 'https://proza.ru/texts/list.html?topic=all&type=selected&year=2022&month=01&day=1'\n",
    "URL_HEAD = 'https://proza.ru/texts/list.html?topic=all&type=selected&year=2020&month=01&day=1'\n",
    "all_year_links = generate_links(URL_HEAD)[:5]\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.41 YaBrowser/21.2.0.1099 Yowser/2.5 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# stop_id = 0\n",
    "stop_id = 704\n",
    "for cur_url in all_year_links:\n",
    "    crawler_current = PageParser(cur_url, 30)\n",
    "    crawler_current.find_articles()\n",
    "    time.sleep(1)\n",
    "\n",
    "    for ind, article_url in enumerate(crawler_current.urls):\n",
    "        parser = ArticleParser(url=article_url, article_id=stop_id+ind+1)\n",
    "        parser.parse()\n",
    "        parser.write_info()\n",
    "\n",
    "    stop_id += crawler_current.id_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45168f3",
   "metadata": {},
   "source": [
    "# News parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91915a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fontanka_reader(path_in, path_out):\n",
    "    text_length = 0\n",
    "    file_ind = 0\n",
    "\n",
    "    for folder in os.listdir(path_in):\n",
    "        gain = 0\n",
    "\n",
    "        for file in Path(os.path.join(path_in, folder)).glob('*.txt'):\n",
    "            if '.csv' in str(file):\n",
    "                continue\n",
    "\n",
    "            cur_text = []\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                for line in f.readlines():\n",
    "                    if '# text =' in line:\n",
    "                        if line[9] == '#':\n",
    "                            continue\n",
    "                        cur_text.append(line.split(' = ')[-1]+' ')\n",
    "\n",
    "            text_length += len(cur_text.split())\n",
    "            gain += len(cur_text.split())\n",
    "\n",
    "            article_txt_name = \"{}_raw.txt\".format(file_ind+1)\n",
    "            with open(os.path.join(path_out, article_txt_name), 'w', encoding='utf-8') as file_out:\n",
    "                file_out.write(' '.join(text_parsed))\n",
    "            file_ind += 1\n",
    "\n",
    "            if gain >= 23000:\n",
    "                break\n",
    "\n",
    "        print('Folder: ', folder, '\\nCorpus gain: ', gain)\n",
    "    return file_ind, text_length\n",
    "\n",
    "\n",
    "def interfax_reader(path_in, path_out, start):\n",
    "    text_length = 0\n",
    "    file_ind = start\n",
    "\n",
    "    for folder in os.listdir(path_in):\n",
    "        gain = 0\n",
    "\n",
    "        if '.txt' in folder:\n",
    "            continue\n",
    "\n",
    "        for file in Path(os.path.join(path_in, folder)).glob('*.txt'):\n",
    "            if '.csv' in str(file):\n",
    "                continue\n",
    "\n",
    "            cur_text = []\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                for line in f.readlines():\n",
    "                    if '# text =' in line:\n",
    "                        if line[9] == '#':\n",
    "                            continue\n",
    "                        cur_text.append(line.split(' = ')[-1]+' ')\n",
    "\n",
    "            text_length += len(cur_text.split())\n",
    "            gain += len(cur_text.split())\n",
    "\n",
    "            article_txt_name = \"{}_raw.txt\".format(file_ind+1)\n",
    "            with open(os.path.join(path_out, article_txt_name), 'w', encoding='utf-8') as file_out:\n",
    "                file_out.write(' '.join(cur_text))\n",
    "            file_ind += 1\n",
    "\n",
    "            if gain >= 36000:\n",
    "                if folder == 'world' and text_length < 250000:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        print('Folder: ', folder, '\\nCorpus gain: ', gain)\n",
    "    return file_ind, text_length\n",
    "\n",
    "\n",
    "def kp_reader(path_in, path_out, start):\n",
    "    text_length = 0\n",
    "    file_ind = start\n",
    "\n",
    "    for file in Path(path_in).glob('*.txt'):\n",
    "        if '.csv' in str(file):\n",
    "            continue\n",
    "\n",
    "        cur_text = []\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                for line in f.readlines():\n",
    "                    if '# text =' in line:\n",
    "                        if line[9] == '#':\n",
    "                            continue\n",
    "                        cur_text.append(line.split(' = ')[-1]+' ')\n",
    "\n",
    "            text_length += len(cur_text.split())\n",
    "\n",
    "        article_txt_name = \"{}_raw.txt\".format(file_ind+1)\n",
    "        with open(os.path.join(path_out, article_txt_name), 'w', encoding='utf-8') as file_out:\n",
    "            file_out.write(' '.join(cur_text))\n",
    "        file_ind += 1\n",
    "\n",
    "        if text_length >= 250000:\n",
    "            break\n",
    "\n",
    "    print('Folder: ', path_in, '\\nCorpus gain: ', text_length)\n",
    "    return file_ind, text_length\n",
    "\n",
    "\n",
    "def lenta_reader(path_in, path_out, start):\n",
    "    text_length = 0\n",
    "    file_ind = start\n",
    "\n",
    "    for file in Path(path_in).glob('*.txt'):\n",
    "        if '.csv' in str(file):\n",
    "            continue\n",
    "\n",
    "        cur_text = []\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                for line in f.readlines():\n",
    "                    if '# text =' in line:\n",
    "                        if line[9] == '#':\n",
    "                            continue\n",
    "                        cur_text.append(line.split(' = ')[-1]+' ')\n",
    "\n",
    "            text_length += len(cur_text.split())\n",
    "\n",
    "        article_txt_name = \"{}_raw.txt\".format(file_ind+1)\n",
    "        with open(os.path.join(path_out, article_txt_name), 'w', encoding='utf-8') as file_out:\n",
    "            file_out.write(' '.join(cur_text))\n",
    "        file_ind += 1\n",
    "\n",
    "        if text_length >= 250000:\n",
    "            break\n",
    "\n",
    "    print('Folder: ', path_in, '\\nCorpus gain: ', text_length)\n",
    "    return file_ind, text_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
